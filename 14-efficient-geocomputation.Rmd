# Efficient Geocomputation

## Prerequisites {-}

## Introduction

So far, this book has demonstrated geocomputation with relatively small data sets, this may have given the impression that geocomputation can be done quickly. However, as you move from examples to real problems you may find that the amount of data and thus the time to process increases significantly. In the worst cases your computer may fail to complete the task, run out of memory, or crash. This chapter will introduce some technique you may use to boost the performance of large or complex geocomputations.

## Benchmarking
When thinking about efficient programming it is important to have a “what works” attitude.  You may come across programmers that insist that “X is always better than Y”, this is rarely true.  Therefore, it is necessary to test different methods and compare their performance.  Even in cases where some methods are intrinsically faster if you have to spend significant amount of your time rewriting your code to save a small amount of the computers time it may not be worth the effort.

Bechnmakring is simple the process to timing how long your code takes, and thus allowing you to work out if chanign the way your code runs has saved any time.

### Sys.Time()
A really simple way to measure the time a chunk of code takes to run is to simply record the time before and after.  The `Sys.Time()` function returns the current date and time.  While the `difftime()` function returns the difference between two times.
```{r}
t1 = Sys.time()
## Arbitrary Code that takes some time to run
x = runif(1000000)
x = sqrt(x)
##
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```
If you run this code several times you will notice that the run time varies slightly. This is due to your computer having other processing to do for other application you may have running or merely the background requirements of the operating system. 
While `Sys.Time()` is very simple it is not a great way to benchmark code as most functions take milliseconds to run not seconds. 

### microbenchmark
The microbenchmark package is designed to give a quick way to test different code chunks and get a measure of the average performance.

```{r}
library(microbenchmark)
x = runif(1000000)
microbenchmark(sqrt(x),  x ^ 0.5, times = 100) #Two methods for calculating the square root of a number
```
Running multiple tests gives greater confidence that the observed different in performance is representative.  In this case ` sqrt(x)` is much faster than ` x ^ 0.5` this illustrates the kinds of small changes that you can make to your code to achieve significant improvements in overall performance.

### Profvis
The profvis package produces graphs showing the time to execute each line of your code. This can be particularly helpful in identifying where you should focus your optimisation efforts.


## Theory

Although this book is not a guide to the details of computer science a little theory will help you understand what makes your code run fast or slow. 

### Order

In computer science problems can be divided by their order using [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation). Without going into too much detail the amount of time an algorithm takes to run changes with the number of elements (n) in the problem. For example if an algorithm takes twice as long to run for twice as much data computer scientists say in runs in linear time or f(n) = O(n).

While it may be inconvenient to wait ten times as long to process ten times as much data, it is usually tolerable.  However, not all algorithms take linear time.  Some take polynomial time f(n) = O(n^c^), where, for example, ten times as much data takes a hundred times as long to process, the worst algorithms take factorial time f(n) = O(n!) (e.g. 5! = 5 x 4 X 3 X 2 X 1), in these problems ten times more data would take 10! = 3,628,800 times longer to process!  There are many more orders that exist, but for now, it will suffice for you to know these three common types.

Knowing what the order of your problem is will help you decide how to approach optimising your code.  Linear time problems are usually those that are independent of each other.  For example `st_buffer()` runs in linear time, as the buffering of point 1 is completely unaffected by the buffering of point 2.  Problems that involve the interaction of each member usually require quadratic time.  For example `st_distance()` runs in polynomial time as the number of distances to calculate increases as the numebr of points increase.

Although algorithms have order, particular problems may not.  This means that it can be possible to take an algorithm that runs in polynomial or factorial time and rewrite it as on that only takes linear time.  Whether this is possible for all algorithms is an ongoing area of research for mathematics and computer science.  A practical example is that of sorting a list of numbers (or anything else) into order.  Simple sorting algorithms such as bubble sort are O(n^2^) while a more efficient algorithms such as insertion sort are O(n).

If your algorithm already runs in linear time, or cannot be rewritten to do so then the only way to improve performance it to make each individual iteration run faster.  These kinds of improvements will scale up to large dataset but may not be meaningful for problems of higher orders.

### Memory Management

Modern computers can read and write from memory incredibly fast.  But it still takes some amount of time.  For many problems loading in the data in and out of memory can take up more time than performing the calculation.  So efficient memory management is essential for well-optimised code.

Probably one of the most common mistakes in R memory management is to repeatedly copy data by using functions such as `rbind()` take this simple example.

```{r}
# A very poorly written piece of code
# Produces a data.frame of numbers 1 to 10 and their squares
t1 = Sys.time()
df = data.frame(a = 1, b = 1 ^2)
for(i in 2:100000){
  df2 = data.frame(a = i, b = i ^2)
  df = rbind(df, df2)
}
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```

Although to the human eye, this code mealy "sticks" the next row of the data frame onto the end of the existing data frame, to the computer there may not be space in the memory to "stick on" the next row. Thus, each time this loop runs the contents of df must be copied to a new place in memory, before the new row can be added.  On a small data frame like this, the time cost is not noticeable but on a large data set, your code can spend the vast majority of its time copying data rather than calculating your results.

The solution to this kind of problem is to use lists.  Lists work differently "under the hood", which means that you can add to the end of a list without copying the rest of the list.

```{r}
# A better piece of code
# Produces a data.frame of numbers 1 to 10 and their squares
library(dplyr)
t1 = Sys.time()
df.list = list()
for(i in 1:100000){
  df2 = data.frame(a = i, b = i ^2)
  df.list[[i]] = df2
}
# df.list is a list of data frames, we must bind each data frame together in a single result
df = bind_rows(df.list) # Uses the dplyr package and is faster than base R do.call("rbind",df.list)
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```

#### Releasing Memory

Another common problem with large geocompuation problems is running out of memory, or in the case of Raster datasets having to write to the hard disk rather than working in RAM. Running out of memeory usually resutls in R crashing or failing to execute fucntions, while using the hard disk resutls in a servere performance penalty.

Although the full details of memeory managment are beyond the scope of this book several useful points are worth remembering.

[1] 32 Bit versions of R (or any other program) can only use up to 4 GB of meory
[2] 32 Bit versions of R on 32 Bit Windows can only use 2-3 GB of memory
[3] 64 Bit versions of R can access all the RAM on your computer (you are unlikly to have more than 16 exabytes of RAM which is the 64 Bit limit)
[4] Use the `rm()` function to remove objects that you no longer need in your code, this will allows R to re use the memoery for other functions
[5] In more complex cases use `gc()` to release memory from R to the operatign system, this is only necessary if your are working with very large files, as R mostly handels this aspect of memeory management on its own.
[6] Consider discarding unneeded data. For example if you are produing a map using boundary data for an external source. That data may come with attributes that have no use to you. Remove them from the data frame after importing them to R to free up memeory for other uses.
[7] Consider geometric simplification, disucssed later in the chapter.


## Iteration with Spatial Data

### Vectorisation (Why Iteration has not come up earlier)
Iteration (doing the same thing multiple times) is extremely common in programming. But can be introduce quite late in the process of learning R. This is because R is a vectorised language, and thus can easily handle long vectors or variables as well as single variables. 
Consider thing simple example:
```{r}
x = 5
sqrt(x)
y = c(5,10,15,20)
sqrt(y)

```
The same is true of many spatial functions such as `st_buffer()` which can create a buffer around a single geometry or many geometries. However in the background the `st_buffer()` function is working along a list of geometries performing the same task again and again. Finally, the function puts all the individual results together into a single object and returns them to you.
Vectorisation in R is very efficient, and very easy to use, but you may eventually come to a problem which cannot be done in a single vectorised step and must instead iterate along a list of geometries.
The reason iteration matters for efficient geocomputation is that price of code that you iterate over may be run thousands or millions of times, thus millisecond gains in a price of code can multiplied to hours or days of processing time by iteration. 
### Is iteration necessary?
As iteration can have potentially high computation costs, it is always worth asking if iteration is necessary. The is an old story in mathematics that as a child Gauss was set a pointless task by his teacher to add the numbers 1 to 100. This was intended to keep the children occupied, and is a good example of an iterative task. 
1 + 2 = 3
3 + 3 = 6
6 + 4 = 10 
And so on
However, so the story goes, Gauss realised that there is short cut. For any list of positive integers from 1 to n:
S = n(n + 1) / 2
There is no need to go into the detail of the mathematics here merely to show that we can benchmark the two solutions in R.  In this case, I have increased the problem from 1:100 to 1:10000 to account of the greater performance of your computer.
```{r}
sum(1:10000) == 10000*(10000+1)/2 # Check the result is the same
microbenchmark(sum(1:10000),  10000*(10000+1)/2, times = 100) # Compare the performance
```
This simple example is to show that if iteration can be avoided it will usually offer far greater performance gains that even the most optimised of iterative processes.

### Iteration with for loops
The simples form of iteration is the for loop.

```{r}
for(i in 1:10){
  print(i)
}
```
If you are not familiar with loops read http://r4ds.had.co.nz/iteration.html#for-loops 
Looping across spatial data using the ***sf*** package is very similar to working with any other form of data in R. Therefore, let introduce a complex spatial problem as a way to show some good and bad ways to solve it.
Using the cycle_hire data from the spData package. Gives me a data.frame of points representing the location of cycle hire stands in London.

```{r}
library(spData)
points <- cycle_hire
plot(points$geometry)
```
The task to perform is:
1)	For points that have no empty spaces buffer a circle around the point with radius equal to the number of bikes;
2)	For points with some empty space construct a square around the point with width equate to the number of bikes;
3)	For point will all empty spaces construct an equilateral triangle around the point with sides length equal to the number of empty spaces;
This is clearly not a very useful task, but is has characteristics of a complex geocomputation. 
1)	It has conditions: the nature of the geocomputation varies based on the input data;
2)	It is based on multiple variables from a data.frame;
3)	It requires a multi-step process;
4)	It requires a non-standard geometric operation (constructing a triangle centred on a point)
Let’s start by constructing the basic structure of a for loop to solve this problem 

```{r}
for(i in seq_along(points$id)){ #Loop along each row of the points data.frame
  #Get the number of bikes and spaces for the current row
  bikes = points$nbikes[i]
  empty = points$nempty[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    print("Make a circle")
  }else if (bikes == 0){
    #Make a triangle
    print("Make a triangle")
  }else if (bikes != 0 & empty != 0){
    #make a square
    print("Make a square")
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }

}

```
So far there is no geocomputation, just the simple logic of which type of shape we should generate and the for loop. Notice that the code fetches the relevant variables from the data.frame and stores them as new variables bikes and empty. If you are working on very large data.frame and need to make multiple checks on a value within your loop, creating a new temporary variable may save time. Also notice that the last else statement concerns a case that should never happen. This can be helpful in debugging complex operation where an unexpected value or combination of values appears within a large data.frame.
Now lets add in the construction of the shapes

```{r}
library(sf)
library(tmap)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)
t1 <- Sys.time()
shapes <- list()
for(i in seq_along(points.proj$id)){ #Loop along each row of the points data.frame
  #Get the number of bikes and spaces for the current row
  bikes = points.proj$nbikes[i]
  empty = points.proj$nempty[i]
  point = points.proj$geometry[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    pol = st_buffer(point, bikes)
    pol = pol[[1]]
  }else if (bikes == 0){
    #Make a triangle
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Calculate x & y coordinate of the corner of the triangle
    a_x = x + empty/2
    a_y = y - empty/(2 * sqrt(3))
    b_x = x - empty/2
    b_y = y - empty/(2 * sqrt(3))
    c_x = x
    c_y = y + ( sqrt(0.75 * empty ^2) - (empty/(2 * sqrt(3))) )
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(a_x, a_y), c(b_x, b_y), c(c_x, c_y), c(a_x, a_y) )))
    
  }else if (bikes != 0 & empty != 0){
    #make a square
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Get the coordiante of the sqaure
    xmin = x - bikes/2
    xmax = x + bikes/2
    ymin = y - bikes/2
    ymax = y + bikes/2
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(xmin, ymin), c(xmin, ymax), c(xmax, ymax), c(xmax, ymin), c(xmin, ymin) )))
    
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }
  
  #Add the results to a list
  shapes[[i]] = pol

}

result = data.frame(id = points.proj$id, geometry = NA)
result$geometry <- st_sfc(shapes)
st_geometry(result) = result$geometry
st_crs(result) = 27700

t2 <- Sys.time()
difftime(t2,t1, units = "secs")
qtm(result)

```
Lets look at each part of the code in turn.
Before the Loop
We convert the points to a projected coordinate system, as this makes subsequent calculations easier, and we do thins outside of the loop as `st_transform` is a vectorised function and we can take advantage of the performance gain from using vectorisation. 
We also create an empty list `shapes` for the results to be inserted into. 
Making a Circle
This is the simplest case a we can simply call the `st_buffer` function. But notice that the class of `pol` is "sfc_POLYGON" "sfc". In other words, it a set of geometries that happens to only contain a single geometry. When we construct the triangles and square later in the code the are single geometries. So for consistency `pol = pol[[1]]` extracts the raw geometry from the list. 
Making the Triangle
Note the use of [] to delve deep into the structure of a spatial object and extract out the raw numerical values. We then construct the x & y coordinate for each point on the triangle. An optional exercise is to establish the geometric rules used to construct the triangle. Finally, the x & y coordinate are assembled into a polygon. Notice that coordinates are not passed directly to `st_polygon` but instead are made into a list of matrices. 
Making the Square
Similar to making the triangle, but with simpler geometric rules.
Exporting the results
Each resulting polygon is added to the list `shapes`. 
After the loop
A data.frame is constructed and the list of geometries are added to the data.frame. `st_geometry(result) = result$geometry` Ideified this column to R as a geometry column and converts from data.frame to a sf data.frame.
As the geometries we created are raw they do not have a CRS, but they used the CRS of the British National Grid in the calculation (27700) so the CRS can be assigned. Note that we assign the British national grid before transforming back to latitude and longitudes (4326).
Finally we check the results using tmap
## Functional Iteration
This code is not optimal there are a number of place where identical calculation are made repeatedly (e.g. `2 * sqrt(3)` ) as well as duplication between the triangle and square part of the code that could be reduced. It will suffice as a test bed to demonstrate techniques for increasing the performance of your code.
### from loops to functions
While loops are conceptually simple to create they are difficult to integrate with other code. Especially if you start having loops within loops. An excellent way to reduce the complexity is to create functions which contain your loops. This may not provide an inherent speed boost but is a pre-requisite for later improvements. For more information on function writing see http://r4ds.had.co.nz/iteration.html#for-loops-vs.functionals
Proper functions should take in inputs and return outputs without having to refer to the outside world. However, the flexibility of R allows functions to access global variable from within a function. This can allow a loop to be turned into a function with minimal rewriting of the code.

```{r}
library(sf)
library(tmap)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)

points2shapes <- function(i){
  #Get the number of bikes and spaces for the current row
  bikes = points.proj$nbikes[i]
  empty = points.proj$nempty[i]
  point = points.proj$geometry[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    pol = st_buffer(point, bikes)
    pol = pol[[1]]
  }else if (bikes == 0){
    #Make a triangle
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Calculate x & y coordinate of the corner of the triangle
    a_x = x + empty/2
    a_y = y - empty/(2 * sqrt(3))
    b_x = x - empty/2
    b_y = y - empty/(2 * sqrt(3))
    c_x = x
    c_y = y + ( sqrt(0.75 * empty ^2) - (empty/(2 * sqrt(3))) )
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(a_x, a_y), c(b_x, b_y), c(c_x, c_y), c(a_x, a_y) )))
    
  }else if (bikes != 0 & empty != 0){
    #make a square
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Get the coordiante of the sqaure
    xmin = x - bikes/2
    xmax = x + bikes/2
    ymin = y - bikes/2
    ymax = y + bikes/2
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(xmin, ymin), c(xmin, ymax), c(xmax, ymax), c(xmax, ymin), c(xmin, ymin) )))
    
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }
  
  return(pol)

}

# We can call the function for any row in the data frame
result <- points2shapes(1)
plot(result)

```

Now we have converted out code into a function which will run for any row, we want to run it for all the rows in our data frame. This can by done in two ways the tradional way is to use the `apply()` family of functions. Or, we can use the newer `map()` famiyl of functions from the `purrr` package.

```{r}
library(sf)
library(tmap)
library(purrr)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)

# using the apply family of functions
t1 <- Sys.time()
results <- lapply(1:nrow(points.proj), points2shapes)# Returns a list of geometries
shapes <- points.proj # Make a copy of the original data frame
shapes$geometry <- st_sfc(results) # Convert the list into a simple features column, and repalce the points with oour new shapes
st_crs(shapes) <- st_crs(points.proj) # THe list of geometrys has no crs, so we must reasign the same crs
t2 <- Sys.time()
difftime(t2,t1, units = "secs")
qtm(shapes)

# using the map family of functions


```

The hisorically lapplys woulc run siginicantly faster than the for loop. However, gradual improvements to how R handels loops have narrowed the gap substantially. Using lapply still has benefits:

[1] Placing code into functions reduces duplication of code and reduces errors
[2] Functions are tidier and don't leave rouge varaibles after running 
[3] Functions are easier to parallelise than loops


## Parallelisation

Computers are very good a doing one thing at a time. When your computer does several things at once, say running and R script and playing music, what it is actually doing is switching between those two task so quickly that you don't notice the gaps. Most modern computer processors now have multiple cores, with 2-4 being common and high end desktops having 8 or more cores. Multiple cores allow multiple task to be done in parallel. So your computer might use one core to play music and another core to run your R script.

R code natually runs as a single "thread", certain problems can be paralleised and thus be done fater than serial problems. Before looking at how to paralleise code, we need to know when to paralleise.

### When is Paralleisation Appropiate

Not all problems are suitable of parellisation. Take the example of making tea, a simple tea making process miight look like:

[1] Put tea in pot
[2] Pour cold water into kettle and turn on kettle
[3] Wait for kettle to boil water
[4] Pour boiling water in to pot
[5] Wait for tea to brew
[6] Pour milk in to cup
[7] Pour tea in to cup
[8] Drink tea

This is largely a serial process, it it impossible to pour the tea into the cup before the kettle has boiled the water. There are some processes that can be done at the same time, for example you could put the tea in the pot while the kettle is boiling the water. An more optimised process might look like:

[1] Pour cold water into kettle and turn on kettle
[2] Put tea in pot
[3] Wait for kettle to boil water
[4] Pour boiling water in to pot
[5] Pour milk in to cup
[6] Wait for tea to brew
[7] Pour tea in to cup
[8] Drink tea

No we have achived some light parellisation by performing some tasks duing the waiting period of boiling the kettle and brewing the tea.But having eight people assigned to each do one of the seven stages would not be faster than having one person do all the stages. But now supposde that one pot of tea can provide four cups of tea and we need to produce 32 cups of tea. Now there may be oppertunities for paralleisation. If we had eight people with eight kettles and eight tea pots they could produce all 32 cups of tea in the time it takes one person to produce four cups of tea.

There is, however, a cost gathering eight people with eight kettles and tea pots into one kitchen is liekly to get crowded. Perhaps they will get in each other way or we might run out of tea cups.

These issues are all true for geocompuation as well as tea, and can be summaried as:

[1] Alway optimise your process before tying to parallise it
[2] Not all process can be parellised, especially if later stages are dependant on earlier stages
[3] Sometimes you can parellise a serial process if you need to do it many times, or it can be divided in chunks (e.g. different regions)
[4] There are costs to parelliseation which may exceed the benefits of doing many tasks at once

With R geompuation parellisation is unlikly to benefit simple processes unless you are handeling million of rows of data. But if you have a more complex task especially if it is the kind of task that may take seconds per row rather than miliseconds you may find that parellisation can provide significant speed boosts.

### Parallel R for Windows, Linux, and Mac

Unfortunatly parellisation works differently on Windows as it does on Linux and Mac computers. Linux and Mac support the `mcapply` function which simplifys the use of parellisation, however this is unaviaible on Windows. As the majority of users are on Windows, this book will focus on the Windows technique (which will also work on Linux and Mac), but non-Windows users may find their code is a little simpler whe using the `mcapply` function.

### parlapply

Remember earlier in the chapter when we converted a loop to a lappy call without gaining a significant boost in speed? `parlapply` is the reason we did this. Put simply it is a parellel version of the `lapply` function. The basic strucutre of a parallel geocomutaion is:

```{r}
library(sf)
library(parallel)

#make a cluster of 3 workers
cl <- makeCluster(3)

# A simple wrapper function containing the parlapply
fun <- function(cl){
  parLapply(cl, 1:nrow(mydata) ,myfunction) #the parlappy just like the lapply by we also specify the cluster
}

#Any packages or data we need to export to the workers
clusterExport(cl, varlist=c("var1", "var2","var3")) # Any data required
clusterEvalQ(cl, {library(sf); source("myscript.R")}) # Any packages and external scripts (i.e. not the script you are currnety running)

# Call the Wrapper Fuucntion and do the geocompuation
result <- fun(cl) # will be a list of lenght = number of workers

# Stop the cluser not that it is complete
stopCluster(cl)
rm(cl,fun) # Clean up
      
# Bind the list of results together
result <- do.call("rbind",result)


```





 





## Simplification of spaitial data

Spatial data can be chalananging to work with becuase it is very large. This increases memory usage and processing time. There are ways to reduce the size of spatial data significantly by reducing the detail and precison of the data. For exmaple if you were producing a map of the world, you would be  probably not require a map accurate to the nearst metre.

### Geometric Simplification

The function `st_simplify()` reduces the number of points used to descibe a shape while preserving the general shape using the [Ramer–Douglas–Peucker algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm).

```{r}
library(spData)
library(sf)
shape = st_transform(lnd[1,], 27700) #Simplification works best on projected coordinate systems
shape = shape[,"geometry"] #Discard the attributes to focus on the geometry
shape.simple1 = st_simplify(shape, preserveTopology = TRUE, dTolerance = 100)
shape.simple2 = st_simplify(shape, preserveTopology = TRUE, dTolerance = 1000)
plot(shape$geometry, border = "black")
plot(shape.simple1$geometry, border = "red", add = T)
plot(shape.simple2$geometry, border = "green", add = T)
object.size(shape)         # 24968 bytes
object.size(shape.simple1) #  5344 bytes
object.size(shape.simple2) #  4688 bytes

```

In this simple example we reduce the size of the geometry by more than a factor of five. Note that `st_simplify` works on a per geometry basis. So if you have to adjacent poygons that share a boorder before simplification they may not be perfectly aligned after simplification.

### Precision Reduction

Have a look as the coordiantes of a single point that makes up polygon we used above

```{r}

print( as.character( shape$geometry[[1]][1][[1]][[1]][1,] ) ) #Extracts the first point in a multipolygon

```

Theses cordinate are accuate to the nearst 0.000000001 metres or 1 nano metre. For most geocomupational purposes this is an unecessary levle of procison. Lets reduce the procision of the data and see how much memoery we can save.

```{r}

shape.simple3 = shape
shape.simple3$geometry <- st_as_binary(shape.simple3$geometry, precision = 10) # SF only allows precison to change during a conversion
shape.simple3$geometry <- st_as_sfc(shape.simple3$geometry)
print( as.character( shape.simple3$geometry[[1]][1][[1]][[1]][1,] ) )
object.size(shape)         # 24968 bytes
object.size(shape.simple3) # 24760 bytes

```

In this case the saving is not very significant, but if you are using a file format that stores geometry as text, for example GeoJSON, the sumulative saving for many shapes can be signfificant.

In both these examples there is a loss of data, you will need to consider carefully the concequences of discarding this data before using these techniques. They can can however be particually useful during disemination. For example when you export and interative map from RStudio the spatial data is stored in the GeoJSON format, which is widely used in web mapping. GeoJSON benefits significantly from the use of these techniques, and thus improved the perfroamcne of your published maps.
