# Efficient Geocomputation

## Prerequisites {-}

## Introduction

So far, this book has demonstrated geocomputation with relatively small data sets, this may have given the impression that geocomputation can be done quickly. However, as you move from examples to real problems you may find that the amount of data and thus the time to process increases significantly. In the worst cases your computer may fail to complete the task, run out of memory, or crash. This chapter will introduce some technique you may use to boost the performance of large or complex geocomputations.

## Benchmarking
When thinking about efficient programming it is important to have a “what works” attitude.  You may come across programmers that insist that “X is always better than Y”, this is rarely true.  Therefore, it is necessary to test different methods and compare their performance.  Even in cases where some methods are intrinsically faster if you have to spend significant amount of your time rewriting your code to save a small amount of the computers time it may not be worth the effort.

Bechnmakring is simple the process to timing how long your code takes, and thus allowing you to work out if chanign the way your code runs has saved any time.

### Sys.Time()
A really simple way to measure the time a chunk of code takes to run is to simply record the time before and after.  The `Sys.Time()` function returns the current date and time.  While the `difftime()` function returns the difference between two times.
```{r}
t1 = Sys.time()
## Arbitrary Code that takes some time to run
x = runif(1000000)
x = sqrt(x)
##
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```
If you run this code several times you will notice that the run time varies slightly. This is due to your computer having other processing to do for other application you may have running or merely the background requirements of the operating system. 
While `Sys.Time()` is very simple it is not a great way to benchmark code as most functions take milliseconds to run not seconds. 

### microbenchmark
The microbenchmark package is designed to give a quick way to test different code chunks and get a measure of the average performance.

```{r}
library(microbenchmark)
x = runif(1000000)
microbenchmark(sqrt(x),  x ^ 0.5, times = 100) #Two methods for calculating the square root of a number
```
Running multiple tests gives greater confidence that the observed different in performance is representative.  In this case ` sqrt(x)` is much faster than ` x ^ 0.5` this illustrates the kinds of small changes that you can make to your code to achieve significant improvements in overall performance.

### Profvis
The profvis package produces graphs showing the time to execute each line of your code. This can be particularly helpful in identifying where you should focus your optimisation efforts.


## Theory

Although this book is not a guide to the details of computer science a little theory will help you understand what makes your code run fast or slow. 

### Order

In computer science problems can be divided by their order using [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation). Without going into too much detail the amount of time an algorithm takes to run changes with the number of elements (n) in the problem. For example if an algorithm takes twice as long to run for twice as much data computer scientists say in runs in linear time or f(n) = O(n).

While it may be inconvenient to wait ten times as long to process ten times as much data, it is usually tolerable.  However, not all algorithms take linear time.  Some take polynomial time f(n) = O(n^c^), where, for example, ten times as much data takes a hundred times as long to process, the worst algorithms take factorial time f(n) = O(n!) (e.g. 5! = 5 x 4 X 3 X 2 X 1), in these problems ten times more data would take 10! = 3,628,800 times longer to process!  There are many more orders that exist, but for now, it will suffice for you to know these three common types.

Knowing what the order of your problem is will help you decide how to approach optimising your code.  Linear time problems are usually those that are independent of each other.  For example `st_buffer()` runs in linear time, as the buffering of point 1 is completely unaffected by the buffering of point 2.  Problems that involve the interaction of each member usually require quadratic time.  For example `st_distance()` runs in polynomial time as the number of distances to calculate increases as the numebr of points increase.

Although algorithms have order, particular problems may not.  This means that it can be possible to take an algorithm that runs in polynomial or factorial time and rewrite it as on that only takes linear time.  Whether this is possible for all algorithms is an ongoing area of research for mathematics and computer science.  A practical example is that of sorting a list of numbers (or anything else) into order.  Simple sorting algorithms such as bubble sort are O(n^2^) while a more efficient algorithms such as insertion sort are O(n).

If your algorithm already runs in linear time, or cannot be rewritten to do so then the only way to improve performance it to make each individual iteration run faster.  These kinds of improvements will scale up to large dataset but may not be meaningful for problems of higher orders.

### Memory Management

Modern computers can read and write from memory incredibly fast.  But it still takes some amount of time.  For many problems loading in the data in and out of memory can take up more time than performing the calculation.  So efficient memory management is essential for well-optimised code.

Probably one of the most common mistakes in R memory management is to repeatedly copy data by using functions such as `rbind()` take this simple example.

```{r}
# A very poorly written piece of code
# Produces a data.frame of numbers 1 to 10 and their squares
t1 = Sys.time()
df = data.frame(a = 1, b = 1 ^2)
for(i in 2:100000){
  df2 = data.frame(a = i, b = i ^2)
  df = rbind(df, df2)
}
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```

Although to the human eye, this code mealy "sticks" the next row of the data frame onto the end of the existing data frame, to the computer there may not be space in the memory to "stick on" the next row. Thus, each time this loop runs the contents of df must be copied to a new place in memory, before the new row can be added.  On a small data frame like this, the time cost is not noticeable but on a large data set, your code can spend the vast majority of its time copying data rather than calculating your results.

The solution to this kind of problem is to use lists.  Lists work differently "under the hood", which means that you can add to the end of a list without copying the rest of the list.

```{r}
# A better piece of code
# Produces a data.frame of numbers 1 to 10 and their squares
library(dplyr)
t1 = Sys.time()
df.list = list()
for(i in 1:100000){
  df2 = data.frame(a = i, b = i ^2)
  df.list[[i]] = df2
}
# df.list is a list of data frames, we must bind each data frame together in a single result
df = bind_rows(df.list) # Uses the dplyr package and is faster than base R do.call("rbind",df.list)
t2 = Sys.time()
difftime(t2,t1, units = "secs")
```

### Parallelisation

Computers are very good a doing one thing at a time. When your computer does several things at once, say running and R script and playing music, what it is actually doing is switching between those two task so quickly that you don't notice the gaps. Most modern computer processors now have multiple cores, with 2-4 being common and high end desktops having 8 or more cores. Multiple cores allow multiple task to be done in parallel. So your computer might use one core to play music and another core to run your R script.

R code natually runs as a single "thread", however later in this chapter we will explore how cenrtain problems can be paralleised and thus be done fater than serial problems.
 

## Iteration with Spatial Data

### Vectorisation (Why Iteration has not come up earlier)
Iteration (doing the same thing multiple times) is extremely common in programming. But can be introduce quite late in the process of learning R. This is because R is a vectorised language, and thus can easily handle long vectors or variables as well as single variables. 
Consider thing simple example:
```{r}
x = 5
sqrt(x)
y = c(5,10,15,20)
sqrt(y)

```
The same is true of many spatial functions such as `st_buffer()` which can create a buffer around a single geometry or many geometries. However in the background the `st_buffer()` function is working along a list of geometries performing the same task again and again. Finally, the function puts all the individual results together into a single object and returns them to you.
Vectorisation in R is very efficient, and very easy to use, but you may eventually come to a problem which cannot be done in a single vectorised step and must instead iterate along a list of geometries.
The reason iteration matters for efficient geocomputation is that price of code that you iterate over may be run thousands or millions of times, thus millisecond gains in a price of code can multiplied to hours or days of processing time by iteration. 
### Is iteration necessary?
As iteration can have potentially high computation costs, it is always worth asking if iteration is necessary. The is an old story in mathematics that as a child Gauss was set a pointless task by his teacher to add the numbers 1 to 100. This was intended to keep the children occupied, and is a good example of an iterative task. 
1 + 2 = 3
3 + 3 = 6
6 + 4 = 10 
And so on
However, so the story goes, Gauss realised that there is short cut. For any list of positive integers from 1 to n:
S = n(n + 1) / 2
There is no need to go into the detail of the mathematics here merely to show that we can benchmark the two solutions in R.  In this case, I have increased the problem from 1:100 to 1:10000 to account of the greater performance of your computer.
```{r}
sum(1:10000) == 10000*(10000+1)/2 # Check the result is the same
microbenchmark(sum(1:10000),  10000*(10000+1)/2, times = 100) # Compare the performance
```
This simple example is to show that if iteration can be avoided it will usually offer far greater performance gains that even the most optimised of iterative processes.

### Iteration with for loops
The simples form of iteration is the for loop.

```{r}
for(i in 1:10){
  print(i)
}
```
If you are not familiar with loops read http://r4ds.had.co.nz/iteration.html#for-loops 
Looping across spatial data using the ***sf*** package is very similar to working with any other form of data in R. Therefore, let introduce a complex spatial problem as a way to show some good and bad ways to solve it.
Using the cycle_hire data from the spData package. Gives me a data.frame of points representing the location of cycle hire stands in London.

```{r}
library(spData)
points <- cycle_hire
plot(points$geometry)
```
The task to perform is:
1)	For points that have no empty spaces buffer a circle around the point with radius equal to the number of bikes;
2)	For points with some empty space construct a square around the point with width equate to the number of bikes;
3)	For point will all empty spaces construct an equilateral triangle around the point with sides length equal to the number of empty spaces;
This is clearly not a very useful task, but is has characteristics of a complex geocomputation. 
1)	It has conditions: the nature of the geocomputation varies based on the input data;
2)	It is based on multiple variables from a data.frame;
3)	It requires a multi-step process;
4)	It requires a non-standard geometric operation (constructing a triangle centred on a point)
Let’s start by constructing the basic structure of a for loop to solve this problem 

```{r}
for(i in seq_along(points$id)){ #Loop along each row of the points data.frame
  #Get the number of bikes and spaces for the current row
  bikes = points$nbikes[i]
  empty = points$nempty[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    print("Make a circle")
  }else if (bikes == 0){
    #Make a triangle
    print("Make a triangle")
  }else if (bikes != 0 & empty != 0){
    #make a square
    print("Make a square")
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }

}

```
So far there is no geocomputation, just the simple logic of which type of shape we should generate and the for loop. Notice that the code fetches the relevant variables from the data.frame and stores them as new variables bikes and empty. If you are working on very large data.frame and need to make multiple checks on a value within your loop, creating a new temporary variable may save time. Also notice that the last else statement concerns a case that should never happen. This can be helpful in debugging complex operation where an unexpected value or combination of values appears within a large data.frame.
Now lets add in the construction of the shapes

```{r}
library(sf)
library(tmap)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)
t1 <- Sys.time()
shapes <- list()
for(i in seq_along(points.proj$id)){ #Loop along each row of the points data.frame
  #Get the number of bikes and spaces for the current row
  bikes = points.proj$nbikes[i]
  empty = points.proj$nempty[i]
  point = points.proj$geometry[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    pol = st_buffer(point, bikes)
    pol = pol[[1]]
  }else if (bikes == 0){
    #Make a triangle
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Calculate x & y coordinate of the corner of the triangle
    a_x = x + empty/2
    a_y = y - empty/(2 * sqrt(3))
    b_x = x - empty/2
    b_y = y - empty/(2 * sqrt(3))
    c_x = x
    c_y = y + ( sqrt(0.75 * empty ^2) - (empty/(2 * sqrt(3))) )
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(a_x, a_y), c(b_x, b_y), c(c_x, c_y), c(a_x, a_y) )))
    
  }else if (bikes != 0 & empty != 0){
    #make a square
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Get the coordiante of the sqaure
    xmin = x - bikes/2
    xmax = x + bikes/2
    ymin = y - bikes/2
    ymax = y + bikes/2
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(xmin, ymin), c(xmin, ymax), c(xmax, ymax), c(xmax, ymin), c(xmin, ymin) )))
    
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }
  
  #Add the results to a list
  shapes[[i]] = pol

}

result = data.frame(id = points.proj$id, geometry = NA)
result$geometry <- st_sfc(shapes)
st_geometry(result) = result$geometry
st_crs(result) = 27700

t2 <- Sys.time()
difftime(t2,t1, units = "secs")
qtm(result)

```
Lets look at each part of the code in turn.
Before the Loop
We convert the points to a projected coordinate system, as this makes subsequent calculations easier, and we do thins outside of the loop as `st_transform` is a vectorised function and we can take advantage of the performance gain from using vectorisation. 
We also create an empty list `shapes` for the results to be inserted into. 
Making a Circle
This is the simplest case a we can simply call the `st_buffer` function. But notice that the class of `pol` is "sfc_POLYGON" "sfc". In other words, it a set of geometries that happens to only contain a single geometry. When we construct the triangles and square later in the code the are single geometries. So for consistency `pol = pol[[1]]` extracts the raw geometry from the list. 
Making the Triangle
Note the use of [] to delve deep into the structure of a spatial object and extract out the raw numerical values. We then construct the x & y coordinate for each point on the triangle. An optional exercise is to establish the geometric rules used to construct the triangle. Finally, the x & y coordinate are assembled into a polygon. Notice that coordinates are not passed directly to `st_polygon` but instead are made into a list of matrices. 
Making the Square
Similar to making the triangle, but with simpler geometric rules.
Exporting the results
Each resulting polygon is added to the list `shapes`. 
After the loop
A data.frame is constructed and the list of geometries are added to the data.frame. `st_geometry(result) = result$geometry` Ideified this column to R as a geometry column and converts from data.frame to a sf data.frame.
As the geometries we created are raw they do not have a CRS, but they used the CRS of the British National Grid in the calculation (27700) so the CRS can be assigned. Note that we assign the British national grid before transforming back to latitude and longitudes (4326).
Finally we check the results using tmap
## Functional Iteration
This code is not optimal there are a number of place where identical calculation are made repeatedly (e.g. `2 * sqrt(3)` ) as well as duplication between the triangle and square part of the code that could be reduced. It will suffice as a test bed to demonstrate techniques for increasing the performance of your code.
### from loops to functions
While loops are conceptually simple to create they are difficult to integrate with other code. Especially if you start having loops within loops. An excellent way to reduce the complexity is to create functions which contain your loops. This may not provide an inherent speed boost but is a pre-requisite for later improvements. For more information on function writing see http://r4ds.had.co.nz/iteration.html#for-loops-vs.functionals
Proper functions should take in inputs and return outputs without having to refer to the outside world. However, the flexibility of R allows functions to access global variable from within a function. This can allow a loop to be turned into a function with minimal rewriting of the code.

```{r}
library(sf)
library(tmap)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)

points2shapes <- function(i){
  #Get the number of bikes and spaces for the current row
  bikes = points.proj$nbikes[i]
  empty = points.proj$nempty[i]
  point = points.proj$geometry[i]
  
  #If Else statement to decide what to do
  if(empty == 0){
    #Make a Circle
    pol = st_buffer(point, bikes)
    pol = pol[[1]]
  }else if (bikes == 0){
    #Make a triangle
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Calculate x & y coordinate of the corner of the triangle
    a_x = x + empty/2
    a_y = y - empty/(2 * sqrt(3))
    b_x = x - empty/2
    b_y = y - empty/(2 * sqrt(3))
    c_x = x
    c_y = y + ( sqrt(0.75 * empty ^2) - (empty/(2 * sqrt(3))) )
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(a_x, a_y), c(b_x, b_y), c(c_x, c_y), c(a_x, a_y) )))
    
  }else if (bikes != 0 & empty != 0){
    #make a square
    #Extract the coordinate from the point
    x = as.numeric(point[[1]])[1]
    y = as.numeric(point[[1]])[2]
    
    #Get the coordiante of the sqaure
    xmin = x - bikes/2
    xmax = x + bikes/2
    ymin = y - bikes/2
    ymax = y + bikes/2
    
    #Make the polygon
    pol = st_polygon(list(rbind( c(xmin, ymin), c(xmin, ymax), c(xmax, ymax), c(xmax, ymin), c(xmin, ymin) )))
    
  }else{
    #This case should never occur, so if it does stop and return an error message
    warning(paste0("Unexpected case for bikes and empty in row ",i))
    stop()
  }
  
  return(pol)

}

# We can call the function for any row in the data frame
result <- points2shapes(1)
plot(result)

```

Now we have converted out code into a function which will run for any row, we want to run it for all the rows in our data frame. This can by done in two ways the tradional way is to use the `apply()` family of functions. Or, we can use the newer `map()` famiyl of functions from the `purrr` package.

```{r}
library(sf)
library(tmap)
library(purrr)
tmap_mode("view")
points.proj <- st_transform(points, 27700) #Convert to a projected coordinate system (British National Grid)

# using the apply family of functions
t1 <- Sys.time()
results <- lapply(1:nrow(points.proj), points2shapes)# Returns a list of geometries
shapes <- points.proj # Make a copy of the original data frame
shapes$geometry <- st_sfc(results) # Convert the list into a simple features column, and repalce the points with oour new shapes
st_crs(shapes) <- st_crs(points.proj) # THe list of geometrys has no crs, so we must reasign the same crs
t2 <- Sys.time()
difftime(t2,t1, units = "secs")
qtm(shapes)

# using the map family of functions


```

On a small dataset such as this you will not notice a significant performance difference for functionalising you code. Let scale up the problem and to see the difference.

```{r}

# make the problem very big by repeateding the data many times
points.proj <- st_transform(points, 27700)

```

This code simply rep


## Parallelisation
